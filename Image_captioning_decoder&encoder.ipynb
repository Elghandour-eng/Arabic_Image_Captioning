{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPGR3KokGqWHSnS0bxp+jgc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Elghandour-eng/Arabic_Image_Captioning/blob/main/Image_captioning_decoder%26encoder.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **`Image Captioning`**"
      ],
      "metadata": {
        "id": "ojFfrTa5imgm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1- Importing Libraries"
      ],
      "metadata": {
        "id": "ulk9PoC5i3lH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install tensorflow"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WXIhcZb5Q_yu",
        "outputId": "0f96b63c-9778-4630-c5e1-d9cae57c2fc7"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.12.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (23.3.3)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.54.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.8.0)\n",
            "Requirement already satisfied: jax>=0.3.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.4.10)\n",
            "Requirement already satisfied: keras<2.13,>=2.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.12.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (16.0.0)\n",
            "Requirement already satisfied: numpy<1.24,>=1.22 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.22.4)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (23.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: tensorboard<2.13,>=2.12 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.12.2)\n",
            "Requirement already satisfied: tensorflow-estimator<2.13,>=2.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.12.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.5.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.32.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.40.0)\n",
            "Requirement already satisfied: ml-dtypes>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from jax>=0.3.15->tensorflow) (0.1.0)\n",
            "Requirement already satisfied: scipy>=1.7 in /usr/local/lib/python3.10/dist-packages (from jax>=0.3.15->tensorflow) (1.10.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (3.4.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (2.27.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (0.7.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (1.8.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (2.3.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (5.3.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow) (1.3.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (3.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.13,>=2.12->tensorflow) (2.1.2)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (0.5.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow) (3.2.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd     # for data manipulation and analysis\n",
        "import numpy as np      # for scientific computing\n",
        "import matplotlib.pyplot as plt     # for plotting graphs\n",
        "import seaborn as sns     # for plotting graphs\n",
        "import tensorflow as tf   #modeling\n",
        "from tensorflow import keras  # modeling\n",
        "import os # for managing directories and paths\n",
        "import re # import regex\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer # import tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences # import pad_sequences"
      ],
      "metadata": {
        "id": "Htq-JhRRi719"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2- Load Data"
      ],
      "metadata": {
        "id": "7Yj0_1xojH0Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "captions_folders = '/flicker8k_text'\n",
        "\n",
        "if not os.path.exists(os.path.abspath('.') + captions_folders):\n",
        "    caption_zip = tf.keras.utils.get_file('Flickr8k_text.zip',\n",
        "                                            cache_subdir=os.path.abspath('.'), \n",
        "                                            origin = 'https://github.com/jbrownlee/Datasets/releases/download/Flickr8k/Flickr8k_Dataset.zip',\n",
        "                                            extract = True)\n",
        "    os.remove(caption_zip) #remove the zipfile since we have extracted it\n",
        "    \n",
        "image_folders = '/flicker8k_Dataset'\n",
        "\n",
        "if not os.path.exists(os.path.abspath('.') + image_folders):\n",
        "    image_zip = tf.keras.utils.get_file('Flickr8k_Dataset.zip',\n",
        "                                            cache_subdir=os.path.abspath('.'), \n",
        "                                            origin = 'https://github.com/jbrownlee/Datasets/releases/download/Flickr8k/Flickr8k_text.zip',\n",
        "                                            extract = True)\n",
        "    os.remove(image_zip) #remove the zipfile since we have extracted it\n",
        "\n",
        "else:\n",
        "    path = os.path.abspath('.') + image_folders\n",
        "    "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sqbkw0LsjECK",
        "outputId": "30280268-ce72-4cd0-d43c-6c692a4c97ef"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://github.com/jbrownlee/Datasets/releases/download/Flickr8k/Flickr8k_Dataset.zip\n",
            "1115419746/1115419746 [==============================] - 9s 0us/step\n",
            "Downloading data from https://github.com/jbrownlee/Datasets/releases/download/Flickr8k/Flickr8k_text.zip\n",
            "2340801/2340801 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class LoadData:\n",
        "    def load_text_doc(self ,file_path):\n",
        "        with open(file_path) as f:\n",
        "            captions = f.read()\n",
        "            return captions\n",
        "    \n",
        "    def image_caption_dict(self ,text):\n",
        "        caption_mappings = {}\n",
        "        lines = text.split('\\n')\n",
        "        \n",
        "        for line in lines :\n",
        "            line_split = line.split('\\t') # split the line by tab delimiter (Tokenize)\n",
        "            if len(line_split) < 2: # skip the lines with less than 2 elements\n",
        "                continue\n",
        "            else :\n",
        "                image_meta , caption = line_split\n",
        "            raw_image_name , caption_number = image_meta.split('#') # split the image name and caption number\n",
        "            image_name = raw_image_name.split('.')[0] # remove the .jpg extension from the image name\n",
        "            \n",
        "            if (int(caption_number) == 0):\n",
        "                caption_mappings[image_name] = [caption] # create a new list for the image name\n",
        "            else :\n",
        "                caption_mappings[image_name].append(caption)\n",
        "            \n",
        "        return caption_mappings\n",
        "    \n",
        "    def train_img_names(self ,file_path):\n",
        "        data = []\n",
        "        with open(file_path) as f:\n",
        "            text = f.read()\n",
        "            lines = text.split('\\n')\n",
        "            \n",
        "            for line in lines :\n",
        "                if len(line) < 1:\n",
        "                    continue\n",
        "                image_name = line.split('.')[0]\n",
        "                data.append(image_name)\n",
        "        return (data) # return a list of image names"
      ],
      "metadata": {
        "id": "diieF_tcsvW0"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_loader = LoadData()\n",
        "\n",
        "dataset_dir = '/content/Flicker8k_Dataset'\n",
        "captions_file_path = '/content/Flickr8k.token.txt'\n",
        "train_file_path = '/content/Flickr_8k.trainImages.txt'\n",
        "\n",
        "\n",
        "captions_txt = data_loader.load_text_doc(captions_file_path)\n",
        "image_caption_dict = data_loader.image_caption_dict(captions_txt)\n",
        "train_img_names = data_loader.train_img_names(train_file_path)"
      ],
      "metadata": {
        "id": "6F2ErtQPL9Fo"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3- Data Preprocessing"
      ],
      "metadata": {
        "id": "ASb_tI2EMEag"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PreProcessimages :\n",
        "    def load_image(self, path):\n",
        "        image = tf.io.read_file(path) # read the image from the path\n",
        "        image = tf.image.decode_jpeg(image, channels=3) # decode the image to a tensor\n",
        "        image = tf.image.resize(image, (299, 299)) # resize the image to the required size\n",
        "        image = tf.keras.applications.inception_v3.preprocess_input(image) # preprocess the image\n",
        "        \n",
        "        return image, path\n",
        "    \n",
        "    def apply_inception_v3(self, ds_dir , train_img_names):\n",
        "        \n",
        "        from tqdm import tqdm # tqdm for progress bar\n",
        "        \n",
        "        model = tf.keras.applications.InceptionV3(include_top=False, weights='imagenet')\n",
        "        new_input = model.input # get the input layer\n",
        "        hidden_layer = model.layers[-1].output # get the output layer\n",
        "        image_features_extract_model = tf.keras.Model(new_input, hidden_layer) # create a new model\n",
        "        trainig_img_path = [ ds_dir + '/'+ img_name + '.jpg' for img_name in train_img_names] # create a list of image paths\n",
        "        encoded_training_data = sorted(set(trainig_img_path)) # remove duplicates\n",
        "        img_ds = tf.data.Dataset.from_tensor_slices(encoded_training_data) # create a dataset from the list of image paths\n",
        "        img_ds = img_ds.map(self.load_image, num_parallel_calls=tf.data.experimental.AUTOTUNE).batch(14) # load the images in batches\n",
        "        \n",
        "        \n",
        "        for img, path in tqdm(img_ds):\n",
        "            batch_features = image_features_extract_model(img)\n",
        "            batch_features = tf.reshape(batch_features, \n",
        "                                        (batch_features.shape[0], -1, batch_features.shape[3])) # reshape the features to (batch_size, 64, 2048)\n",
        "        \n",
        "        for bf, p in zip(batch_features, path):\n",
        "            path_of_feature = p.numpy().decode(\"utf-8\")\n",
        "            np.save(path_of_feature, bf.numpy()) # save the features to a file"
      ],
      "metadata": {
        "id": "ngbNd8kBs7Ar"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img_preprocess = PreProcessimages()\n",
        "img_preprocess.apply_inception_v3(dataset_dir , train_img_names)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1nE50qT2syKV",
        "outputId": "f91af99c-4359-431d-9419-5221aa5a4da5"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/inception_v3/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "87910968/87910968 [==============================] - 0s 0us/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 429/429 [01:35<00:00,  4.50it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re # import regex\n",
        "\n",
        "class PrepocessCaptions:\n",
        "    \n",
        "    def clean_captions_text(self ,parsed_img_captions_dict):\n",
        "        \n",
        "        for key , captions_text in parsed_img_captions_dict.items():\n",
        "            for i ,caption in enumerate(captions_text):\n",
        "                caption_nopunct = re.sub(r'[^a-zA-z0-9]','',caption.lower()) # remove punctuations\n",
        "                clean_words = [word for word in caption_nopunct.split() if ((len(word)>1) and (word.isalpha()))] # remove single letter words\n",
        "    \n",
        "    \n",
        "    def add_token(self, captions):\n",
        "        for i, caption in enumerate(captions):\n",
        "            captions[i] = 'startseq ' + caption + ' endseq'\n",
        "        return (captions)\n",
        "    \n",
        "    \n",
        "    def img_subset_data_dict(self, img_dict, img_names):\n",
        "        data_dict = { img_name : self.add_token(captions) for img_name, captions\n",
        "                     in img_dict.items() if img_name in img_names}\n",
        "        return (data_dict)\n",
        "    \n",
        "    \n",
        "    def flat_caps(self, data_dict):\n",
        "       return { caption for key, captions in data_dict.items() for caption in captions}\n",
        "   \n",
        "   \n",
        "    def caps_max_word_len(self, caps):\n",
        "        return max(len(caption.split()) for caption in caps)\n",
        "    \n",
        "    \n",
        "    def apply_tokenizer(self, data_dict):\n",
        "        caps = self.flat_caps(data_dict)\n",
        "        max_cap_len = self.caps_max_word_len(caps)\n",
        "        \n",
        "        \n",
        "        tokenzier = Tokenizer()\n",
        "        tokenzier.fit_on_texts(caps)\n",
        "        vocab_size = len(tokenzier.word_index) + 1\n",
        "        \n",
        "        return (tokenzier , vocab_size , max_cap_len)\n",
        "    \n",
        "    \n",
        "    def pad_text(self, parsed_img_caps_dict, max_length):\n",
        "        paddeed_cap_text = pad_sequences([parsed_img_caps_dict], maxlen=max_length, padding='post')[0]\n",
        "        \n",
        "        return (paddeed_cap_text)\n",
        "    \n",
        "    def data_prep(self, data_dict, img_dir ,tokenizer, max_len, vocab_size):\n",
        "        X, y = list(), list()\n",
        "        \n",
        "        for image_name, captions in data_dict.items():\n",
        "            iamge_name = img_dir + image_name + '.jpg' \n",
        "            for caption in captions:\n",
        "                word_idxs = tokenizer.texts_to_sequences([caption])[0]\n",
        "                pad_idxs = self.pad_text(word_idxs, max_len)\n",
        "                X.append(image_name)\n",
        "                y.append(pad_idxs)\n",
        "                \n",
        "        return X, y "
      ],
      "metadata": {
        "id": "a0ssvqbYx0Zt"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "caps_preprocess = PrepocessCaptions() # create an instance of the class\n",
        "\n",
        "caps_preprocess.clean_captions_text(image_caption_dict)  # clean the captions text\n",
        "data_dict_train = caps_preprocess.img_subset_data_dict(image_caption_dict, train_img_names) # create a dictionary of image names and their captions\n",
        "tokenizer, vocab_size, max_length = caps_preprocess.apply_tokenizer(data_dict_train) # apply the tokenizer on the training data\n",
        "train_X, train_y = caps_preprocess.data_prep(data_dict_train, dataset_dir, tokenizer, max_length, vocab_size) # prepare the training data"
      ],
      "metadata": {
        "id": "mH9-0yhcRcLn"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def npy_loader(img_name, captions):\n",
        "    img_tensor = np.load(img_name.decode('utf-8')+'.npy')\n",
        "    return img_tensor, captions\n",
        "\n",
        "def prepare_tf_dataset(Buffer_size, Batch_size, X_train, y_train):\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
        "    dataset = dataset.map(lambda item1, item2: tf.numpy_function(\n",
        "                npy_loader, [item1, item2], [tf.float32, tf.int32]),\n",
        "                num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "    dataset = dataset.shuffle(Buffer_size).batch(Batch_size)\n",
        "    dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "    return dataset"
      ],
      "metadata": {
        "id": "ZqMAmX6QR7z6"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "a_rgJpKEUGVv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}